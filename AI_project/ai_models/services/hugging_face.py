"""
Hugging Face Transformers ë¡œì»¬ ì„œë¹„ìŠ¤
ëª¨ë“  AI ëª¨ë¸ì„ ì„œë²„ì—ì„œ ì§ì ‘ ì‹¤í–‰
"""
from transformers import pipeline
from PIL import Image
import io
import torch

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = 0 if torch.cuda.is_available() else -1


class HuggingFaceService:
    """Hugging Face Transformers ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    # ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìºì‹± (í•œ ë²ˆë§Œ ë¡œë“œ)
    _image_caption_pipeline = None
    _speech_to_text_pipeline = None
    _audio_classification_pipeline = None
    _summarization_pipeline = None
    _text_generation_pipeline = None
    
    @classmethod
    def _get_image_caption_pipeline(cls):
        """ì´ë¯¸ì§€ ìº¡ì…”ë‹ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if cls._image_caption_pipeline is None:
            print("ğŸ“¥ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._image_caption_pipeline = pipeline(
                "image-to-text",
                model="Salesforce/blip-image-captioning-base",
                device=device
            )
        return cls._image_caption_pipeline
    
    @classmethod
    def _get_speech_to_text_pipeline(cls):
        """ìŒì„±â†’í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if cls._speech_to_text_pipeline is None:
            print("ğŸ“¥ ìŒì„± ì¸ì‹ ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._speech_to_text_pipeline = pipeline(
                "automatic-speech-recognition",
                model="openai/whisper-tiny",  # tiny: ë¹ ë¦„, small: ì •í™•
                device=device
            )
        return cls._speech_to_text_pipeline
    
    @classmethod
    def _get_audio_classification_pipeline(cls):
        """ìŒì•… ì¥ë¥´ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if cls._audio_classification_pipeline is None:
            print("ğŸ“¥ ìŒì•… ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._audio_classification_pipeline = pipeline(
                "audio-classification",
                model="MIT/ast-finetuned-audioset-10-10-0.4593",
                device=device
            )
        return cls._audio_classification_pipeline
    
    @classmethod
    def _get_summarization_pipeline(cls):
        """í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if cls._summarization_pipeline is None:
            print("ğŸ“¥ ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._summarization_pipeline = pipeline(
                "summarization",
                model="facebook/bart-large-cnn",
                device=device
            )
        return cls._summarization_pipeline
    
    @classmethod
    def _get_text_generation_pipeline(cls):
        """í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if cls._text_generation_pipeline is None:
            print("ğŸ“¥ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._text_generation_pipeline = pipeline(
                "text-generation",
                model="gpt2",
                device=device
            )
        return cls._text_generation_pipeline
    
    @staticmethod
    def image_to_caption(image_bytes):
        """
        ì´ë¯¸ì§€ â†’ ìº¡ì…˜ ìƒì„±
        
        Args:
            image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            
        Returns:
            dict: {'caption': str, 'success': bool, 'error': str}
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipe = HuggingFaceService._get_image_caption_pipeline()
            result = pipe(image)
            
            caption = result[0]['generated_text']
            
            return {
                'caption': caption,
                'success': True,
                'error': None
            }
                
        except Exception as e:
            return {
                'caption': None,
                'success': False,
                'error': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }
    
    @staticmethod
    def audio_to_text(audio_bytes):
        """
        ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜
        
        Args:
            audio_bytes: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„°
            
        Returns:
            dict: {'text': str, 'success': bool, 'error': str}
        """
        try:
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipe = HuggingFaceService._get_speech_to_text_pipeline()
            result = pipe(audio_bytes, return_timestamps=True)
            
            text = result['text']
            
            return {
                'text': text,
                'success': True,
                'error': None
            }
                
        except Exception as e:
            return {
                'text': None,
                'success': False,
                'error': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }
    
    @staticmethod
    def classify_audio_genre(audio_bytes):
        """
        ìŒì•… ì¥ë¥´ ë¶„ë¥˜
        
        Args:
            audio_bytes: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„°
            
        Returns:
            dict: {'genres': list, 'success': bool, 'error': str}
        """
        try:
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipe = HuggingFaceService._get_audio_classification_pipeline()
            result = pipe(audio_bytes)
            
            # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            genres = result[:5]
            
            return {
                'genres': genres,
                'success': True,
                'error': None
            }
                
        except Exception as e:
            return {
                'genres': None,
                'success': False,
                'error': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }
    
    @staticmethod
    def summarize_text(text):
        """
        í…ìŠ¤íŠ¸ ìš”ì•½
        
        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: {'summary': str, 'success': bool, 'error': str}
        """
        try:
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipe = HuggingFaceService._get_summarization_pipeline()
            
            # BARTëŠ” 1024 í† í° ì œí•œ
            if len(text.split()) > 500:
                text = ' '.join(text.split()[:500])
            
            result = pipe(
                text,
                max_length=130,
                #min_length=30,
                do_sample=False
            )
            
            summary = result[0]['summary_text']
            
            return {
                'summary': summary,
                'success': True,
                'error': None
            }
                
        except Exception as e:
            return {
                'summary': None,
                'success': False,
                'error': f'ìš”ì•½ ì˜¤ë¥˜: {str(e)}'
            }
    
    @staticmethod
    def generate_image_prompt(summary):
        """
        ìš”ì•½ â†’ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            summary: ìš”ì•½ í…ìŠ¤íŠ¸
            
        Returns:
            dict: {'prompt': str, 'success': bool, 'error': str}
        """
        try:
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipe = HuggingFaceService._get_text_generation_pipeline()
            
            input_text = f"Create a podcast cover description: {summary[:100]}\nDescription:"
            
            result = pipe(
                input_text,
                max_new_tokens=50,
                repetition_penalty=1.2,
                no_repeat_ngram_size=3,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=50256,
                eos_token_id=50256,
                return_full_text=False,
            )
            
            prompt = result[0]['generated_text'].strip()
            
            # ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ìš”ì•½ ì‚¬ìš©
            if len(prompt) < 10:
                prompt = f"Podcast cover art: {summary[:100]}"
            
            return {
                'prompt': prompt,
                'success': True,
                'error': None
            }
                
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìš”ì•½ ì‚¬ìš©
            return {
                'prompt': f"Podcast cover: {summary[:100]}",
                'success': True,
                'error': None
            }
    
    @staticmethod
    def create_podcast(audio_bytes):
        """
        íŒŸìºìŠ¤íŠ¸ ë©”ì´ì»¤ - ë³µí•© íŒŒì´í”„ë¼ì¸
        ìŒì„± â†’ í…ìŠ¤íŠ¸ â†’ ìš”ì•½ â†’ í”„ë¡¬í”„íŠ¸
        
        Args:
            audio_bytes: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„°
            
        Returns:
            dict: {
                'transcript': str,
                'summary': str,
                'prompt': str,
                'success': bool,
                'error': str,
                'step': str
            }
        """
        result = {
            'transcript': None,
            'summary': None,
            'prompt': None,
            'success': False,
            'error': None,
            'step': None
        }
        
        # 1ë‹¨ê³„: ìŒì„± â†’ í…ìŠ¤íŠ¸
        print("ğŸ¤ 1ë‹¨ê³„: ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
        step1 = HuggingFaceService.audio_to_text(audio_bytes)
        if not step1['success']:
            result['error'] = step1['error']
            result['step'] = '1ë‹¨ê³„ (ìŒì„±â†’í…ìŠ¤íŠ¸)'
            return result
        
        result['transcript'] = step1['text']
        
        # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ìš”ì•½
        print("ğŸ“ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘...")
        step2 = HuggingFaceService.summarize_text(step1['text'])
        if not step2['success']:
            result['error'] = step2['error']
            result['step'] = '2ë‹¨ê³„ (í…ìŠ¤íŠ¸ ìš”ì•½)'
            return result
        
        result['summary'] = step2['summary']
        
        # 3ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±
        print("ğŸ¨ 3ë‹¨ê³„: ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        step3 = HuggingFaceService.generate_image_prompt(step2['summary'])
        result['prompt'] = step3['prompt']
        
        result['success'] = True
        print("âœ… íŒŸìºìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")
        
        return result